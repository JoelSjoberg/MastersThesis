Before a deep learning model is created it is necessary to have extensive knowledge about the data in the project. As certain tumors may be heterogeneous\cite{friedmann2014glioblastoma}, their biological information may make them hard to group which would make model generalization difficult. The analysis explained here is to determine whether Raman spectrum can sufficiently sort the different samples into the sample classes (LGm1 - 6). In this chapter the data available to the project is examined in greater detail; details for how the Raman spectrum were prepared is given to document the preprocessing of spectrum for future use. The chapter begins by describing the mathematical representation of the samples. The number of samples is deemed too small for use in a deep learning model, an explanation is given for how each sample may be separated into individual spectras after which an argument is made in favor of this representation. This separation yields a drastic increase in the number of training examples available which reinforces the plausibility of deep learning models in this context. The chapter then proceeds to explain how the data is to be balanced. An unbalanced dataset would likely invoke bias into the deep learning model which would introduce uncertainty in it's desired predictive capabilities. This is done by duplicating samples in sample-classes which are underrepresented in the dataset. Furthermore, this balancing is performed to maintain majority and minority classes which retains some distributional information from the original dataset. Certain problematic samples are also discarded in this section to alleviate memory issues which would otherwise be present on a modern computer. The chapter continues with feature selection which is also performed to extract the features best descriptive of the different classes with respect to the other classes. This step further aids in alleviating memory issues and run time, the necessity is further motivated by comparison of the results from the extraction. The chapter ends with an analysis of each sample using hierarchical clustering with complete linkage in an attempt to detect and remove eventual outliers within the samples themselves.


\section{Data Representation}
The data consists of the Raman-spectrum extracted from the tissue of glioma tumors from 45 patients. Multiple samples of tissue was extracted from the same patient in some cases, yielding several samples for the respective patient. To maintain separation among the patients the samples are sorted by the patient to whom they belong. This is necessary at the exploration stage since the data at one point will be divided into training and testing examples. The testing data will consist of unique patients to avoid scenarios in which the model is familiar with a patients tumor sample. This structure also allow for ease in handling the amount of patients in the sample-classes. There is also large variation with regard to the sample shape within the data. Each sample is a 3-dimensional array of shape $(w, h, 1738)$ where $w$ and $h$ are the width and height of the sample respectively. This formalization is necessary, as width and height have non-zero variance among different samples. The shape is a result of how the tissue was scanned. In each case the tissue was placed inside the instrument and scanned successively from side to side. This makes it possible to display each sample as an image, where the third dimension(denoted above by $1738$) could serve as a color value. The number $1738$ is constant through all samples and represent the length of the Raman-spectra, each element a unique frequency. Furthermore each element inside these arrays is a real number without clear bounds. The largest absolute element found within is $79427.0625$, some values are negative which is confirmed by the providers to have significance for the projects purpose. The project aims to predict which subdivision the spectra belong to by feeding in one of these samples i.e. one vector of shape $(1, 1738)$. This is motivated by Liu et al.\cite{liu2017deep} who managed to get satisfactory performance by training a model on raw spectra. To further alleviate memory issues, feature extraction is performed to reduce the sice of the spectra to $(1, 70)$ by extracting the 70 features most capable of characterizing the data. This process is explained in detail in a later section. The provider further states that the spectra may be separated to individual spectrum as their alignment do not add any predictive power. This separation increase the dataset from $45$ patients to \textbf{(NUMBERHERE!!)} spectra, which is a sufficient number of datapoints for a deep learning model. Displaying each sample is possible by visualizing each spectra as a line, each patient has $w * h$ spectra. Before the lines are drawn the maximum absolute value of each frequency within the entire dataset is found. Using these, the frequencies of each spectra may be normalized to have a maximum value of 1 and a minimum value of -1. By normalization the plots may be compared to identify outliers and determine necessary preprocessing measures. Appendix \ref{appendix:spectraplot} contains a comparison by the common pattern these samples portray compared to one of the samples which will be removed from the dataset. This comparison is only one method of detecting outliers. The reason for this samples removal, along with the removal of other problematic samples is further motivated in the section below.

\section{Data preparation}
The preparation of the data is vital to form non-biased models, it is equally important to guarantee that the model is able to generalize to all possible tumor samples. This means the model is expected to perform well on previously unseen data. Should this be impossible the resulting predictive model will fail to grasp the necessary features relevant to each sample-class. The model would in this respect overfit to the data used to train it. To avoid this we preform qualitative analysis on the data to determine the plausibility of such a model. Each sample is categorized according to their subdivision, there are six distinct subdivisions defined by Ceccarelli et al.\cite{cellsubsets}, the labels are denoted by LGm1 - 6.

\subsection{Balancing}
The balancing stage is of tremendous importance, should the model be made from the data without balancing, there is great chance the model would tend towards predictions for the majority class(LGm2). The model will as a consequence of it's learning-algorithm become biased towards certain predictions \textbf{PROVE THIS!}. Over exposure to examples of a certain class will force the model to associate features with that class which in turn redirects focus from the classes for which that feature could be significant. The data suffers heavily from this problem, a table displaying the distribution of the spectra in each class is shown in Table \ref{table:1}.
\\

\begin{table}[h!]
\centering
 \begin{tabular}{||c c c c c c c||} 
 \hline
 Class & LGm1 & LGm2 & LGm3 & LGm4 & LGm5 & LGm6 \\ [0.5ex] 
 \hline\hline
 \# of samples & 5& 16 & 7 & 13 & 15 & 5 \\ 
 \hline
 \# of spectra & 37319 & 210586 & 39636 & 50660 & 62256 & 20176 \\
 \hline
 percentage & 9\%& 50\% & 9\% & 12\% & 15\% & 5\% \\
 \hline

\end{tabular}
\caption{Table showing the distribution of data in the initial dataset, the majority class is LGm2 and minority is LGm6. The classes must be expanded to balance the data.}
\label{table:1}
\end{table}

\newpage
Table \ref{table:1} shows the per class separation in the data, the header row shows the labels of each class. The first row shows the number of samples belonging to each class, these are the tumors which will be analyzed. The second row display the total number of spectra across each class, these must be considered when balancing is performed note that there are an equal amount of samples in LGm1 and LGm6 but have a different number of spectra. This is due to the size of each sample drawn from the tumor. The last row shows the percentage each class makes of the entire dataset. Initially LGm2 is the majority class while LGm6 is the minority, consisting of only $5$\% of the entire dataset. Some samples prove to be problematic as their size make them too big to load into memory. Some even included what appears to be erroneous readings. The plot of one erroneous example is shown in appendix \ref{appendix:spectraplot}. For simplicity, these are removed from the analysis, some included a great number of spectra which cause memory errors when they are plotted. The maximum is also present in these plots and as can be seen in appendix \ref{appendix:spectraplot} the spectra appear to be erroneous. While there are methods of improving their shape, there is enough data to move to the next stage even after their removal. Following the removal of the problematic samples the data is now represented in Table \ref{table:2}
\\

\begin{table}[h!]
\centering
 \begin{tabular}{||c c c c c c c||} 
 \hline
 Class & LGm1 & LGm2 & LGm3 & LGm4 & LGm5 & LGm6 \\ [0.5ex] 
 \hline\hline
 \# of samples & 5& 11 & 4 & 13 & 15 & 5 \\ 
 \hline
 \# of spectra & 37319 & 71846 & 14896 & 50660 & 62256 & 20176 \\
 \hline
 percentage & 15\%& 28\% & 6\% & 20\% & 24\% & 8\% \\
 \hline

\end{tabular}
\caption{The data distribution after removing problematic samples}
\label{table:2}
\end{table}


Before the data is balanced, the testing data is selected and separated from the rest. This is done by separating at least one patient and all their samples from the rest of the data. This way it will be possible to test if the model is overfit to the patients in training and if the patient samples are heterogeneous with respect to the other samples of the same class. Balancing the classes which contain less elements by a factor larger than or equal to two compared to the majority class (LGm2) is done by repeating the spectrum in each sample by that factor. Following this method the majority class will stay the majority which can be crucial provided the sample pattern is similar to the set of all other unseen samples. The resulting dataset is gained by doubling the samples in LGm1, quadrupling the samples in LGm3 and tripling the samples in LGm6. The distributions of the final training dataset is shown in Table \ref{table:3}.
\\
\\
\begin{table}[htb]
\centering
 \begin{tabular}{||c c c c c c c||} 
 \hline
 Class & LGm1 & LGm2 & LGm3 & LGm4 & LGm5 & LGm6 \\ [0.5ex] 
 \hline\hline
\# of spectra & 22374	& 51698	& 11296	& 34276	& 43892	& 12976 \\
 \hline 
 Factor  & 2 & 1 & 4 & 1 & 1 & 3 \\
 \hline 
 \# of spectra & 44748 & 51698 & 45184 & 34276 & 43892 & 38928 \\
 \hline
 percentage & 17\%& 20\% & 17\% & 13\% & 17\% & 15\% \\
 \hline

\end{tabular}
\caption{Distribution of the training data}
\label{table:3}
\end{table}

Table \ref{table:3} shows the number of spectra before multiplying on row one and the number of spectra after on row three. The precentages on row four shows the distribution on the modified dataset. The percentages are now closer in scope,  meaning each class is now in relative balance to the other classes.

\subsection{Clustering}

To ensure each sample include relevant information requires intuitive analysis. During extraction it is possible droplets of certain fluids were present, it is also possible the laser hit the tissue at a thin point which would scan the plastic underneath. This data must be removed to avoid model dependency on erroneous spectra. To find and remove the erroneous spectra, a clustering method is used. A suggested method is the k-means clustering algorithm by MacQueen \cite{macqueen} performed on every spectra on a sample by sample analysis. The resulting clustering may be displayed as a 2-dimensional image, some of which are shown in \textbf{APPENDIX!!}. K-means do however suffer heavily from outlier influences, as these outliers may be present at this stage, it is necessary to consider other methods. Hierarchical clustering is a clustering method which avoids outlier dependency by separating each individual spectra into its own cluster and then reducing the number of clusters depending on the linkage type. Complete linkage is used to ensure diversity among the clusters, measured by euclidean distance\cite{scikit}. Each spectra is now labeled by a class number, all labels may be displayed as a two dimensional image due to the shape of the samples. Examples of such images are shown in appendix \ref{appendix:hierimg0}. The cluster shapes suggest certain outliers within all patients, these outliers are removed accordingly to retain the tissue information exclusively. \textbf{Note: I am still unsure of this step, the clusters seem to also capture what could be perfectly normal tissue. Raise this concern with supervisors, it is crutial to get this part right so as to present rigorous results. I have saved images using alternative distance metrics however they seem to largely suffer from the same problem (some even seem to recreate the tumor image entirely)}

\subsection{Feature selection}

\textbf{Note: Discuss the following with supervisors: feature selection is now done before clustering. The reason I describe clustering first is that I think it would be best to perform clustering first and then extract the features. This extraction did yield features which made the clustering results "make sense" so to speak. However the outliers (eventual plastic or other non-tumor substance) is present during this extraction, meaning non-tissue is taken into consideration during the process. Would it not be better to extract them after removing non-tumor samples? Then, the yielded features might be different! If this is not the case we can proceed without concern, but what if this fails?}
\\

Each number in the spectra is a frequency at which the scattered light is gathered. This light is expected to be sufficient for predicting the methylation-type of the tumor-tissue. Due to the number of spectra inside each sample memory quickly becomes an issue, it is therefore relevant to examine which frequencies would have the most impact on classification. For this reason the best features are extracted with SelectKBestfeatures \cite{scikit}. The 70 best features were extracted from each spectra which is shown to be appropriate to recreate the original tumorshape by performing clustering 