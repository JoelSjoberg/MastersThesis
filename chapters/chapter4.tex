Following the curation of the data in the previous chapter, the number of spectra available in each data set decrease by approximately $25\%$. The distribution of the data remains the same, as the amount of spectra within each category retain the relative sizes to the other categories. In this chapter we present the proposed pre-processing methods for the data. We evaluate the data curation and pre-processing techniques by initializing a deep learning model and training it to predict the desired category of each sample. The results are then presented and analyzed.

We proceed to make the data suitable for application with machine learning methodology. The first method of pre-processing is baseline correction. The baseline of a spectrum is the minimum value each wavelength frequency has. The extraction of the spectral signal from the spectrometer may, depending on the settings of the instrument, produce different minimal values for each frequency. This may lead to smaller spikes appearing in otherwise "flat" areas of the spectrum. Baseline correction alleviates these issues by removing the excess baseline in all frequencies of the spectra, which simplifies analysis among the different frequencies. In context of the Raman spectra available in this project, the baseline by which the spectra are skewed is non-linear, and appears to have a polynomial baseline. This is apparent from the wave-like shape of all spectra, where frequencies are minimal relative to the rest of the frequencies which portray spikes in the spectral wave. In turn, some methods require a polynomial grade which is an approximation of the polynomial wave the baseline appears to originate from. This approximation of the polynomial grade may present issues, as it appears to require analysis of each spectrum in separation from the others. Nor is there any indication that the polynomial grade is uniform across all spectra in the data set. To avoid approximation of the polynomial grade we utilize the ZhangFit-method, which has support for Python through the BaselineRemoval library. The method utilizes the adaptive iteratively reweighted penalized least squares (airPLS) algorithm which iteratively approximates the baseline of the spectrum given without additional information regarding the structure of the spectrum \cite{zhang2010baseline}. For some spectra within the data set, the algorithm reaches the maximal amount of allowed iterations before the spectra are returned, This occurs in approximately $1000$ spectra. However, this setback is ignored, as it is less than one percent of the training spectra. 

Machine learning methods are susceptible to spectra which have large variance among values. Frequencies inside the individual spectra vary tremendously, as spikes within each spectrum may differ from the baseline by a value of $10,000$ and more in many cases. It is clear that some method of normalization is needed to make the spectra applicable with machine learning methods. We opt to use z-score standardization and apply it to the frequencies within the training set. Standardization is performed as follows: Compute the mean and standard deviation of all frequencies through the whole training set. This results in two lists of length $1738$ where the indices of those lists correspond to the mean and standard deviation of the respective frequencies. Each value of the spectra is then standardized by applying z-score standardization to each frequency by using the corresponding mean and standard deviation from the initialized lists. This method is preferred as it brings smaller values close to zero on each frequency while maintaining the larger values for the bigger spikes on the spectra within the training set. The features are however not sufficiently scaled down as many frequencies still exceed a value of $60$ in some spectra.

In further attempts to scale the data into a range which works better for machine learning methods, we also apply normalization by the maximum absolute value. Following standardization, the maximum absolute value of each frequency is measured and stored in a list. The data set is then normalized by dividing each value in each frequency by the corresponding maximum absolute value stored. This retains the sparsity of the data while scaling all values down to a range between negative one and one.

We also define an augmentation method to better regularize the learning process. We train the model by selecting an equal amount of random spectra from each category for every batch used for training, this is done on the training set. Every batch will be balanced as a result, which avoids the problem of unbalanced data sets. The batch is then augmented before the batch is passed to the pre-processor. Augmentation is performed by adding a skewed line to the batch of data which introduces a random, linear baseline to each spectrum in the batch. Each frequency in each spectra in the batch is then given additive, normally distributed noise which is drawn from $1738$ normal distributions, each generated from the means and standard deviations extracted from the data in the pre-processing step. This ensures the noise is within range of the usual values present for the respective frequency. To allow the model a chance to learn the real data patterns, data augmentation is skipped randomly with $30\%$ probability. The augmentation and pre-processing pipelines are displayed in Appendix \ref{appendix:prep}

We create a deep convolutional neural network model and train it using the training set. The model architecture is based on the architecture presented by Liu et al. \cite{liu2017deep} and is five layers deep, consisting of two convolutional layers and three dense layers (one of which is the output layer). The input is always a one-dimensional vector consisting of $1738$ elements (one neuron for each spectral frequency). The one-dimensional convolutional layers consist of $16$ and $32$ kernels respectively; both layers use the leakyReLU activation function and the kernel sizes are $21$ and $16$ respectively. Both layers use batch normalization and max pooling with a size of two. The model then flattens the result from the latter convolutional layer to allow for dense layers. Following the flattening layer, a dropout is provided with a dropout of $20\%$. The two dense layers before the output layer are sized $128$ and $32$ respectively and both use the tanh activation function. Both layers use batch normalization and the latter layer is followed by a dropout of $40\%$ followed by Gaussian noise generated from a normal distribution with a mean of zero and a standard deviation of $0.15$. The output consists of six neurons and uses the softmax activation function to form a probability distribution of the input signal. The loss is then calculated using categorical cross-entropy which is optimized using the Adam optimizer with a learning rate of $0.003$. All layer weights are initialized randomly from a normal distribution with a mean of zero and a standard deviation of $0.1$. After compilation, the model consists of $1,742,374$ total parameters, of which $416$ are non-trainable. A figure displaying the architecture is shown in Appendix \ref{appendix:model}.

Several trials on the data suggest that the size of this architecture is arbitrary and provide little in terms of performance regarding the predictions of the model. We also create variations of the architecture, some of these variations are made with one type of activation function (the activation functions we test are tanh, LeakyReLU, ReLU and sigmoid). Changes to the convolutional layers are also arbitrary; strides and pooling sizes larger than two have been tested. The change to the convolutional layers drastically decreases the amount of learnable parameters in the model which makes the model faster to train. Before the model is initialized we set the random seed in numpy, os.environ and tensorflow to 6 (a value chosen arbitrarily). The resetting of the seeds is important, as it makes the random initializations predictable and, as a consequence, reproducible. The model is trained on balanced batches which consists of $600$ spectra ($100$ spectra per unique category). The batch is generated and augmented before being pre-processed and given to the model where it is then trained on for a duration of two epochs, the entire model trained in this fashion for $40$ epochs.

The first test we perform is to deduce whether the data can be learned by the model or not. We perform this test by separating the training set into two smaller distinct sets. Before separation, the entire set is sorted into a random order which scatters the spectra within randomly; the model is then trained on the first set and evaluated on the other. In this setting, the model manages to learn and generalize well to both halves of the training data. We manage to get these results by using the current pre-processing method discussed earlier in this chapter. Omitting the maximum absolute value normalization method when training in this setting prolongs training drastically and prevents model generalization for the unseen part of the training set. While the accuracy for all spectra is insufficiently low (approximately $82\%$ after training), letting the model train for longer periods of time appear to lead to better results. The overwhelming majority of correct predictions suggest the model is well suited for predicting the categories given all spectra from a sample. More spectra given to the model increases the probability of correctness in the prediction. The model even performs well on unseen spectra (drawn from the same sample used for training) suggesting homogeneity among spectra originating from the same sample. However this does not hold true for the validation data consisting of entire unseen samples. The predictions for the spectra within the validation data are sporadically scattered across many classes. LGm2 is severely underrepresented in the predictions generated by the model and few predictions appear correct through the majority of categories. LGm1 is the only category which is correctly predicted by the model. All other categories fail to be accurately represented, even the majority predictions fail as the majority of predictions are focused on other categories. The confusion matrices of the predictions for the training set and validation set are displayed in Appendix \ref{appendix:confm_half}. In hope of alleviating this issue, we rejoin the split training set into one complete data set and resolve to training on the entire training set while using the validation set for validating the performance of the model.

The model is fully capable of generalizing to the training set which is consistent with the previous result. The number of epochs must be increased further for the model to adequately learn the training data. We therefore increase the number of epochs from $40$ to $80$ which allows the model accuracy to reach over $90\%$ accuracy on the training data. However, the accuracy is not reflected in the validation data. As in the setting with the split training data, the model achieves approximately $30\%$ accuracy on the validation data. This result is constant through multiple runs in which we change the model in various ways, i.e., change of activation functions, layer sizes and amounts and regularization layers such as dropout and Gaussian noise. By examining the confusion matrix made out of the predicted values and expected values we discover that the model tends to produce predictions mainly for categories LGm1, LGm4, LGm5 and LGm6. The model is seldom able to perform satisfactory predictions for more than two categories. The categories the model predicts seem to be a result of the order in which the model encounters spectra from the training set (which is dependent on the mini-batch parameter given to the model when training starts). The fact that the model can generalize to unseen training data but not to the pre-processed validation data suggests that the spectra are heterogeneous among the samples which makes it hard to generalize to all categories. How to alleviate this issue and other possible sources this problem stems from is a speculative matter and elaborated upon in the next chapter.

