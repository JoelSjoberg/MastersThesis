Following the curation of the data in the previous chapter, the number of spectra available in each data set decrease by approximately $25\%$. The distribution of the is however the same, as the amount of spectra within each category retain the relative size to the other categories. In this chapter we present proposed pre-processing methods for the data. We evaluate the data curation and pre-processing techniques by initializing a deep learning model and training it to predict the desired methylation types of each sample. The results are then presented and analyzed.

We proceed to make the data suitable for application with machine learning methodology. The first method of pre-processing is baseline correction. The baseline of a spectrum is the minimum value each wavelength frequency has. The extraction of the spectral signal from the spectrometer may, depending on the settings of the instrument, produce different minimal values for each frequency. This may lead to smaller spikes appearing in otherwise "flat" areas of the spectra. Baseline correction alleviates these issues by removing the excess baseline in all frequencies of the spectra, which simplifies analysis among the different frequencies. In context of the Raman spectra available in this project, the baseline by which the spectra are skewed is non-linear, and instead appear to have a polynomial baseline. This is apparent from the wave-like shape of the spectra where frequencies are minimal relative to the rest of the frequencies which portray spikes in the spectral wave. In turn some methods require a polynomial grade which is an approximation of the polynomial wave the baseline appears to originate from. This approximation of the polynomial grade may present issues, as it appears to require analysis of each spectrum in separation from the others. Nor is there any indication that the polynomial grade is uniform across all spectra in the data set. To avoid approximation of the polynomial grade we utilize the ZhangFit-method which has support for Python through the BaelineRemoval library. The method utilizes the adaptive iteratively rewheighted penalized least squares (airPLS) algorithm which iteratively approximates the baseline of the spectrum given without additional information regarding the structure of the spectrum. For some spectra within the data set, the algorithm reaches the maximal amount of allowed iterations before the spectra are returned, This occurs in approximately $1000$ spectra. However, this setback is ignored as it is less than one percent of the training spectra. 

Machine learning methods are susceptible to spectra which have large variance among values. Frequencies inside the individual spectra vary tremendously, as spikes within each spectrum may differ from the baseline by a value of $10,000$ and more in many cases. It is clear that some method of normalization is needed to make the spectra applicable with machine learning methods. We opt to use \textit{z-score} standardization and apply it to the frequencies within the training set. Standardization is performed as follows: Compute the mean and standard deviation of all frequencies through the whole training set. This results in two lists of length $1738$ where the indices of those lists corresponds to the mean and standard deviation of respective frequencies. Each value of the spectra are then standardized by applying \textit{z-score} standardization to each frequency by using the corresponding mean and standard deviation from the initialized lists. This method is preferred as it brings smaller values close to zero on each frequency while maintaining the larger values for the bigger spikes on the spectra within the training set. The features are however not sufficiently scaled down as many frequencies still exceed a value of $60$ in some spectra.

In further attempt to scale the data into a range which works better for machine learning methods, we also apply normalization by the maximum absolute value. Following standardization, the maximum absolute value of each frequency frequency is measured and stored in a list. The data is then normalized by dividing each value in each frequency by dividing the value by the corresponding maximum absolute value. This retains the sparsity of the data while scaling all values down to a range between negative one and one.


Omitting the normalization step when training on training data with training data as validation did not allow the model to generalize. Using maximum abs. norm. allowed the model to learn and generalize among the training data quickly.