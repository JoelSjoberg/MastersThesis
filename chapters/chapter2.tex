Within this chapter can be found the essential mathematical theory on which this thesis is based. The concepts are considered fundamental for understanding the methods applied in our project. It begins by 
covering necessary mathematical theory required to understand data representations and handling within Machine Learning (ML). It then proceeds by defining common concepts in machine learning and the subject of supervised and unsupervised learning.


\section{Mathematical Foundations}

\section{Machine Learning}

Machine learning is the practice of computing models for relationships between sets of data. The field has garnered significant interest within akademia and industry alike due to the promising result in a number of applications. Within the field there are mainly two paradigms for learning: Supervised learning (using labeled data to approximate models) and unsupervised learning (finding patterns within the data itself). 

Models are used to great length within many scientific domains. Though each domain has defined this term differently, the definitions in the context of machine learning shall be used. In this context, a model is a collection of vector transformations which may be performed on any input vector $x$ to produce a prediction $y'$

\vspace{5mm}

\textbf{Definition: } A model is an approximation of a desired function $f$ which produces relevant results based on human definitions.  

\vspace{5mm}

Mathematically a model may be represented as a collection of numbers $M$ which may in turn be used to compute $f$ for any given example.
In the context of machine learning a set of parameters may be tuned during a learning process (or training process). These parameters are combined with samples of data through some mathematical procedure to effectively model a distribution from which the data was extracted. The equation below is an example of a n-dimensional object.
$$x_0\theta_0 + x_1\theta_1 + ... + x_n\theta_n$$

\subsection{K-means Clustering}

Clustering is an unsupervised learning method whose primary use is in grouping sets of data. In this thesis we consider a fundamental version of such a clustering algorithm called \textit{K-means clustering}. The following is the formal definition of \textit{K - means clustering} as defined by MacQueen\cite{macqueen}. Given a set of $N$-dimensional points $E_N$ and a desired amount of partitions $k$ in said population, partition the elements of $E_N$ into a partitioned set $S = \{S_1, S_2, ... S_k\}$. The partitioning of $E_N$ is performed by initializing $k$  $N$-dimensional points as randomly selected points within $E_N$. We define the set $V$ with elements $v$ where $v_i$ is the i:th cluster center where $i \in [0, k]$. The partitioning of the elements $x \in E_N$ into their respective partition $S_i$ is performed by computing the closest cluster center $\forall_{x \in E_N}$. Let $T_i$ be the set of $x \in E_N$ such that the distance from the element to the relevant cluster is minimal, $T_i$ is defined by formula \ref{eqn:Ti}.

\begin{equation}
\label{eqn:Ti}
T_i = \{x : x \in E_N | (\forall_{j \in [0, k]/i } |x - v_i| \leq |x - v_j|)\} 
\end{equation}

For centers who share equal distance to any given $x$ the cluster with the smallest index is chosen as the containing set. The partitions $S_i \in S$ are defined by formula \ref{eqn:Si}

\begin{equation}
\label{eqn:Si}
S_i = T_i \cap \bigcap_{j=0}^{(i-1)} S_j^c
\end{equation}

A consequence to this definition is that outliers have a potential to drastically change the quality of the cluster outcomes\cite{chawla2013k}. To remedy this and the stochastic nature of the initialization process, the method is run several times on the same dataset, yielding the resulting clusters with minimal inertia. The problem \textit{K-means clustering} attempts to solve is proven to be NP-hard\cite{chawla2013k}\cite{mahajan2009planar} but the algorithm itself has a time complexity of $O(n^2)$\cite{pakhira2014linear}.


\subsection{Hierarchical clustering}

Hierarchical clustering is a clustering method which is less suceptible to outliers compared to K-means. The method produces clusters by iteratively combining the closest clusters according to the linkage criterion. The two primary strategies for forming clusters are agglomerative and divisive. Agglomerative clustering initializes one cluster for each data point and combines them in a hierarchy according to the linkage criterion until all clusters are part of the hierarchy. Divisive strategies process in counter to the agglomerative strategies by initializing one universal cluster for all data points and then separate the points into distinct clusters according to the linkage criterion. The method proceeds until all data points are separated to their own cluster within the unifying hierarchy. The project described in this thesis uses the agglomerative strategy. All strategies depend on the distance measure and linkage criterion\cite{murtagh1983survey}.

\subsubsection{Distance metrics}
Let $u$ and $v$ be vectors of the same dimension $n$. The euclidean distance(alternatively L2) measure can be used to measure distance between the vectors in euclidean space. It is calculated by taking the square root of the summed squared distance between each vector element in each vector. Formally expressed in equation \ref{eqn:euclid}.

\begin{equation}
\label{eqn:euclid}
d(u, v) = \sqrt{\Sigma_i (u_i - v_i)^2} 
\end{equation}

The manhattan distance(alternatively L1) metric is also a viable alternative if the distance is to be measured in blocks. The distance is akin to finding a shortest path among blocks and is therefore calculated by summing the absolute distance of every element between every vector. Formally expressed in equation \ref{eqn:manhattan}.

\begin{equation}
\label{eqn:manhattan}
d(u, v) = \Sigma_i |u_i - v_i|
\end{equation}

Cosine similarity measures similarity between vector angles and suits situations where certain vectors are expected to be similar. Should the vectors be sizable in terms of dimensionality, this method will yield varying results, especially if the elements have significant variance in each dimension. It is calculated by dividing the sum of each element product in each vector by the product of the square roots of each summed and squared vector element. Formally expressed in equation \ref{eqn:cosine}.

\begin{equation}
\label{eqn:cosine}
d(u, v) = \frac{\Sigma_i u_iv_i}{\sqrt{\Sigma_i u_i^2}\sqrt{\Sigma_i v_i^2}}
\end{equation}

\subsubsection{Linkage Criteria}

Furthermore the algorithm uses a linkage criterion to decide which clusters to merge in the agglomerative method(the divisive version uses it to select clusters which should be separated). While many linkage criteria exist, only those available in SKlearns' implementation are explained in this section\cite{scikit}. These criteria consider the positional values of the elements within each cluster and with the distance metric, measures a distance between the clusters i.e. The linkage criterion decides the points between which the distance is measured. Single linkage searches for the element in each cluster that are closest to the elements in every other cluster. The two clusters including the elements that are closest to each other are then merged, this criterion is formally expressed by equation \ref{eqn:single}.

\begin{equation}
\label{eqn:single}
min(d(u_n, v), d(u_m, v))
\end{equation}

Complete linkage works in contrast by searching for the elements which have a maximum distance between each other. After finding the smallest distance of all these, the method merges the clusters. Formally expressed in equation \ref{eqn:complete}

\begin{equation}
\label{eqn:complete}
max(d(u_n, v), d(u_m, v))
\end{equation}

\textbf{TODO: Add formal linkage definitions}
Ward linkage merges the two clusters which have minimal variance. This requires calculating the variance among all elements between two clusters for every combination of clusters within the data. Average linkage calculates the average between all elements for every combination of clusters and merges the ones corresponding to the minimal distance.

\subsection{Feature Selection}

In many cases the data available contains numerous features. This is necessary, as gathering enough features is essential to building a sufficient classifier. However as the number of features increases so does the requirement for training data. To avoid this expanding dependence on data it is necessary to strip the data of certain features which possess minimal description of the data in question or lack that description entirely \cite{dash1997feature}. These features which possess the expressive information are not always trivial, which further motivates the use for machine learning to find these features.

\section{Deep Learning}

The field of Artificial Intelligence is founded on the notion of designing algorithms for solving problems. The field encountered tremendous progress in \textbf{[FIND YEAR, AI FOUNDATIONS]} referred to by \textbf{[NAME]} as the "look ma, no hands" era of Artificial Intelligence. One such method which have proven useful for these tasks is the practice of approximating models through Artificial Neural Networks.

\subsection{Artificial Neural Networks}
Artificial Neural Networks ("ANNs") have been used to great success during the 20th century [\textbf{Source Here}]. With the use of ANNs
several fields including Natural Language Processing, Encoding and Image classification have undergone revolutionary leaps in performance regarding optimization due to the predictive power of these networks [\textbf{Source Here}]. At the same time they are heavily criticized for their complexity, yielding a structure much more akin to a so called \textit{"black box"} than a reliable and deterministic method for prediction[\textbf{Source Here}]. This complexity is due to numerous different structural typologies available at present and an awesome number of tuned parameters which are modified with the goal of minimizing the predictive error [\textbf{Source Here}].

A consequence of this is hard skepticism in regards to the correctness of their functionality within practical use. While these models have shown great promise when compared to their human counterparts, the question remain whether or not perfect performance can be yielded from the constructed models.\\

\textbf{Definition 1.} Training an ANN is allowing minuscule changes through the randomly initialized structure in order to approximate a collection of nested functions $$f_n(f_{n-1}(...f_1(X)))$$ 

\subsection{Computational Representations}
The initial purpose of ANNs was to create a computational model of the human cortex which took the form of the McCulloch, Pitts neuron. The multilayer perceptron (MLP) introduced in \textbf{[year here]} formed the basic structure which would become ANNs. 


