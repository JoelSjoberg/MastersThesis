In this chapter we review the concepts on which this thesis is based. We first cover the statistical theory for understanding the methods of analysis used in Chapter 3 and use of Machine Learning (ML) in Chapter 4. We then proceed by reviewing common concepts and specific methods within machine learning employed in this project.


\section{Statistics}

In this section we discuss the primary statistical concepts required for understanding the methods used in this thesis. The first concepts necessary for the later chapters are the mean and the standard deviation. The mean is a value used to describe the average value of a collection of values (A collection can refer to lists, arrays, sets etc.). The mean is an important concept in statistics as it is often used as a characteristic of the elements found in the collection under analysis. The mean is calculated by the sum of the each element in the collection divided by the number of elements in said collection. The mean $\mu$ of a list of $n$ values $L$, is calculated as expressed in equation \ref{eqn:mean}.

\begin{equation}
\label{eqn:mean}
\mu = \sum_{i=1}^n \frac{L_i}{n}
\end{equation}

The mean of a collection is usually used in association with the standard deviation. The standard deviation is the square root of the variance of that collection. The variance is an expression for how much the elements in the collection are scattered from the mean, calculating the square root then produces a value expressing the dispersion of the variance itself. The variance is calculated by measuring the squared distance between each element within the collection and the mean, divided by the number of elements in the collection subtracted by one. The standard deviation $\sigma$ is calculated as expressed in equation \ref{eqn:std}.

\begin{equation}
\label{eqn:std}
 \sigma = \sqrt{\frac{\sum_{i = 1}^{n} (\mu - L_i)^2}{n-1}}
\end{equation}

\subsection{The standard deviation test}
Within this project, the mean and standard deviation are used to detect outliers in a collection of data points. The Gaussian distribution (also called normal distribution) is used in association with the mean and the standard deviation. If the frequency which elements appear within that collection are more likely to be close to the mean, we say that the elements within that collection are normally distributed. If an element differs from the mean by more than three standard deviations, the possibility of that element not belonging to that distribution is extremely high. Such elements, which should not belong to the collection, are called outliers. The method which detects outliers by measuring the distance between each element and the mean is commonly referred to as the standard deviation test (SDT). The test assumes that elements within the collection are normally distributed with some mean $\mu_c$ and some standard deviation $\sigma_c$. By computing the \textit{z-score} of the elements, the data is transformed into a standardized form through standardization. \textit{Z-score} standardization subtracts the mean from all elements in the collection and divides the difference by the standard deviation as expressed in equation \ref{eqn:zscore}.

\begin{equation}
\label{eqn:zscore}
 \frac{L_i - \mu} {\sigma}
\end{equation}

Following standardization, the entire collection will have a mean of zero and a standard deviation of one. Each element in the collection can then be measured by removing elements from the standardized collection which have an absolute value on 3 or higher.

\subsection{The interquartile range method}
Provided the data is not drawn from a normal distribution, SDT will not work sufficiently well for detecting outliers.  An alternative method suited for outlier detection is the interquartile range method (IRM). IRM is based on analyzing the data by its median, which is the center of the sorted data, it is applied by sorting the data in ascending order and organizing the data into four percentiles, each percentile containing $25\%$ of the data.  Two quartiles center around $50\%$ of the data, this center is referred to as the interquartile range. IQM requires two values from the data collection. One is the highest value of the $25$th percentile and the highest value of the $75$th percentile. The first value is gained by taking the biggest value of the $q_25$ first elements from the sorted collection, where $q_25$ is $25\%$ of the number of elements in $L$ (calculated by $0.25 \cdot |L|$). The last percentile is gained in similar fashion, with the exception that the highest value is taken from the first $75\%$ of the sorted data. Two "cut-off" points are then defined, by multiplying each of the two data values by a constant $k$ (which is called the "cut-off" constant). Elements can then be labeled as outliers if those elements fall below the lower "cut-off" point or above the higher "cut-off" point.

\subsection{Analysis of variance}

We also need the Analysis of variance (ANOVA) method in this project. ANOVA is a method which determines the significance of certain features in a set.

\section{Machine Learning}

\textbf{TODO: Rewrite this section to better match contents, might even be unnecessary if DL is not mentioned in the end}
Machine learning is the practice of computing models for relationships between sets of data. The field has garnered significant interest within academia and industry alike due to the promising result in applications for which deterministic algorithms have proven difficult or impossible to make. There are two paradigms for learning: Supervised learning (using labeled data to approximate models) and unsupervised learning (finding patterns within the data itself). 

Models are used to great length within many scientific domains. Though each domain has defined this term differently, the definitions in the context of machine learning shall be used. In this context, a model is a data structure made out of constant parameters which may be performed on any input vector $x$ to produce a prediction $y$

\vspace{5mm}

\textbf{Definition: } A model is an approximation of a desired function $f$ which produces relevant results based on human definitions.  

\vspace{5mm}

Mathematically a model may be represented as a collection of numbers $M$ which may in turn be used to compute $f$ for any given example.
In the context of machine learning a set of parameters may be tuned during a learning process (or training process). These parameters are combined with samples of data through some mathematical procedure to effectively model a distribution from which the data was extracted. The equation below is an example of a n-dimensional object.
$$x_0\theta_0 + x_1\theta_1 + ... + x_n\theta_n$$

\subsection{K-means Clustering}

Clustering is an unsupervised learning method whose primary use is in grouping sets of data. In this thesis we consider the \textit{K-means clustering} algorithm. The following is a formal definition of \textit{K - means clustering} as defined by MacQueen \cite{macqueen}. Given a set of $N$-dimensional points (where $N \in \mathbb{N}$) $E_N$ and a desired amount of partitions $k$ of $E_N$, partition the elements of $E_N$ into $k$ sets, the subsets are stored in a superset $S$ such that $S = \{S_1, S_2, ... S_k\}$. The partitioning of $E_N$ is performed by randomly initializing $k$  $N$-dimensional points as randomly selected points within $E_N$. We define the set $V$ with elements $v$ where $v_i$ is the i:th cluster center where $i \in [0, k]$. The partitioning of the elements $x \in E_N$ into their respective partition $S_i$ is performed by computing the closest cluster center $\forall_{x \in E_N}$. Let $T_i$ be the set of $x \in E_N$ such that the distance from the element to the relevant cluster is minimal, $T_i$ is defined by formula (\ref{eqn:Ti}).

\begin{equation}
\label{eqn:Ti}
T_i = \{x : x \in E_N | (|x - v_i| \leq |x - v_j|)\}  0 \leq j \leq k \wedge j \in \mathbb{N} 
\end{equation}

For centers who share equal distance to any given $x$ the cluster with the smallest index is chosen as the containing set. Let $S_i^c$ denote the complement of set $S_i$, then the partitions $S_i \in S$ are defined by formula (\ref{eqn:Si}).

\begin{equation}
\label{eqn:Si}
S_i = T_i \cap \bigcap_{j=1}^{(i-1)} S_j^c
\end{equation}

A consequence to this definition is that outliers have a potential to drastically change the quality of the cluster outcomes \cite{chawla2013k}. To remedy this and the stochastic nature of the initialization process, the method is run several times on the same dataset, yielding the resulting clusters with minimal inertia. The problem \textit{K-means clustering} attempts to solve is proven to be NP-hard \cite{chawla2013k, mahajan2009planar} but the algorithm itself has a time complexity of $O(n^2)$ \cite{pakhira2014linear}.


\subsection{Hierarchical clustering}

Hierarchical clustering is a clustering method which has a deterministic process. Each cluster formed is based on the entire dataset in contrast to \textit{K-means} which approximates clusters by performing small changes to the cluster centers. The method produces clusters by iteratively combining the closest clusters according to the given linkage criterion (defined in the sections below). The two primary strategies for forming clusters are \textit{agglomerative} and \textit{divisive}. Agglomerative clustering initializes one cluster for each data point and combines them in a hierarchy according to the linkage criterion until all clusters are part of the hierarchy. Divisive strategies initializes one universal cluster for all data points and proceeds to separate the points into distinct clusters according to the linkage criterion. The method proceeds until all data points are separated to their own cluster within the unifying hierarchy. The project described in this thesis uses the \textit{agglomerative} strategy. All strategies depend on the distance measure and linkage criterion \cite{murtagh1983survey}.

\subsubsection{Distance metrics}
Let $u$ and $v$ be vectors of the same dimension $n$. The \textit{Euclidean distance} (also called \textit{L2-distance}) measure can be used to measure distance between the vectors in euclidean space. The \textit{Euclidean distance} is calculated by equation (\ref{eqn:euclid}).

\begin{equation}
\label{eqn:euclid}
d(u, v) = \sqrt{\Sigma_i (u_i - v_i)^2} 
\end{equation}

The \textit{Manhattan distance} (also called \textit{L1-distance}) metric is also a viable alternative if the distance is to be measured in blocks. The distance is akin to finding a shortest path among blocks and is therefore calculated as expressed in equation (\ref{eqn:manhattan}).

\begin{equation}
\label{eqn:manhattan}
d(u, v) = \Sigma_i |u_i - v_i|
\end{equation}

\textit{Cosine similarity} measures similarity between vector angles and suits situations where certain vectors are expected to be similar. Should the vectors be sizable in terms of dimensionality, this method will yield varying results, especially if the elements have significant variance in each dimension. It is calculated as expressed in equation (\ref{eqn:cosine}).

\begin{equation}
\label{eqn:cosine}
d(u, v) = \frac{\Sigma_i u_iv_i}{\sqrt{\Sigma_i u_i^2}\sqrt{\Sigma_i v_i^2}}
\end{equation}

\subsubsection{Linkage Criteria}
In order to measure distance between clusters it is essential to know between which points the distance should be measured, since clusters often consist of several points. Linkage criteria describes the method for determining how the distance metric will be applied. SKlearn define four criteria in the documentation \cite{scikit}: Single linkage, complete linkage, average linkage and ward linkage. Depending on which criterion is applied the results may differ considerably, it is therefore vital to have a formal understanding of their application and consequences.

Single linkage goes through each pair of clusters measuring the distance among all points within one with respect to the other. The distance between these clusters is determined to be the distance between the two closest points. Let $U$ be the elements in the first cluster and $V$ be the elements of the second. The distance between the first and the second cluster is defined formally in equation (\ref{eqn:single}).

\begin{equation}
\label{eqn:single}
d(U, V) = \forall_{u, v \in U, V} min(d(u, v))
\end{equation}

Single linkage tend to produce trivial results, forging a hierarchy in a chain where individual elements slowly merge with the bigger cluster. In contrast complete linkage considers the largest distance between two points for every pair of clusters. The distance between two clusters then become the distance between the points which are the furthest apart. Formally expressed in equation (\ref{eqn:complete}).

\begin{equation}
\label{eqn:complete}
d(U, V) = \forall_{u, v \in U, V} max(d(u, v))
\end{equation}

By considering the largest possible distance between two clusters it bypasses the setback by single linkage, allowing more clusters to form before merging into one unifying cluster. Average linkage calculates the average between all elements for every pair of clusters and merges the ones possessing to the minimal average distance. Formally described by equation (\ref{eqn:average}).

\begin{equation}
\label{eqn:average}
d(U, V) = \frac{1}{|U||V|}\sum\limits_u^U \sum\limits_v^V  d(U_u, V_v)
\end{equation}

Ward linkage represents distance by how much the summed square would increase by merging them. The method aims to merge the clusters such that the within cluster variance is minimal. Let $c_a$ be the center of cluster a, then ward linkage is expressed formally by equation (\ref{eqn:ward}) \cite{shalizi2009distances}.


\begin{equation}
\label{eqn:ward}
d(U, V) = \frac{|U||V|}{|U|-|V|}||c_U - c_V||^2
\end{equation}



\subsection{Feature Selection}

In many cases the data available contains numerous features, which often helps to building sufficient classifiers as the model may find non-trivial patterns among the features. To avoid expanding the dependence on large datasets, it is often necessary to strip the data of certain features which possess minimal correlation to other features or which lack that correlation entirely \cite{dash1997feature}. Features that possess the necessary expressive information are not always trivial, there are several ways in which they may be found. \textbf{TODO: Expand this, it's been 3 weeks already!}

\section{Deep Learning}

The field of Artificial Intelligence is founded on the notion of designing algorithms for solving problems. The field encountered tremendous progress in \textbf{[FIND YEAR, AI FOUNDATIONS]} referred to by \textbf{[NAME]} as the "look ma, no hands" era of Artificial Intelligence. One such method which have proven useful for these tasks is the practice of approximating models through Artificial Neural Networks.

\subsection{Artificial Neural Networks}
Artificial Neural Networks ("ANNs") have been used to great success during the 20th century [\textbf{Source Here}]. With the use of ANNs
several fields including Natural Language Processing, Encoding and Image classification have undergone revolutionary leaps in performance regarding optimization due to the predictive power of these networks [\textbf{Source Here}]. At the same time they are heavily criticized for their complexity, yielding a structure much more akin to a so called \textit{"black box"} than a reliable and deterministic method for prediction[\textbf{Source Here}]. This complexity is due to numerous different structural typologies available at present and an awesome number of tuned parameters which are modified with the goal of minimizing the predictive error [\textbf{Source Here}].

A consequence of this is hard skepticism in regards to the correctness of their functionality within practical use. While these models have shown great promise when compared to their human counterparts, the question remain whether or not perfect performance can be yielded from the constructed models.\\

\textbf{Definition 1.} Training an ANN is allowing minuscule changes through the randomly initialized structure in order to approximate a collection of nested functions $$f_n(f_{n-1}(...f_1(X)))$$ 

\subsection{Computational Representations}
The initial purpose of ANNs was to create a computational model of the human cortex which took the form of the McCulloch, Pitts neuron. The multilayer perceptron (MLP) introduced in \textbf{[year here]} formed the basic structure which would become ANNs. 


