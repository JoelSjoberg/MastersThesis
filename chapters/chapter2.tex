In this chapter we review the concepts on which this thesis is based. We first cover the statistical concepts and unsupervised methods necessary for understanding the analysis performed in Chapter 3. We then proceed by reviewing common concepts and specific methods within machine learning which is the central theme in Chapter 4.% We then proceed by introducing statistical concepts required for understanding the methods employed in the project. We then review common concepts and specific methods within machine learning which is the central theme in this thesis.


\section{Statistics}

In this section we discuss the basic statistical concepts required for understanding the methods used in this thesis. We first explain the mean and the standard deviation. The mean is a value used to describe the average value of a population. A population is the term used to describe the complete collection of elements in some context e.g. all people alive, all numbers in an interval etc. The mean is an important concept in statistics as it is often used as a characteristic of the elements found in the population under analysis when its elements are quantitative \cite{rowntree1981statistics}. The mean is calculated by the sum of each element divided by the number of elements in said population. The mean $\mu$ of a population of $n$ numbers $L = \{L_1, ..., L_n\}$, is calculated as expressed in formula (\ref{eqn:mean}).

\begin{equation}
\label{eqn:mean}
\mu = \sum_{i=1}^n \frac{L_i}{n}
\end{equation}

The mean of a population is often used in association with the standard deviation. The standard deviation is the square root of the variance of that population. The variance is an expression for the scaled summed squares of differences from the mean of the entire population. Calculating the square root then produces a value expressing the dispersion of elements within the population around the mean. The variance is calculated by measuring the summed squared distance between each element within the population and the mean, divided by the number of elements in the population to scale the sum. In the case where the entire population is impossible to analyze or unknown, samples are extracted from the population (i.e. subgroups of elements randomly taken from the population). The sample variance changes slightly form the population variance; it divides the summed squares of differences by the number of elements in the sample subtracted by one. Subtracting the original denominator by one is called "Bessel's correction". The correction is based on the fact that, if the population mean is unknown (as is often the case when samples are gathered), then the mean used in calculating the variance will only be an estimation. Subtraction by one is performed to avoid bias towards the sample mean and get an unbiased estimation of the population variance \cite{so2008sample, nobach2020practical}. The standard deviation $\sigma$ of a sample is thus calculated as expressed in formula (\ref{eqn:std}).

\begin{equation}
\label{eqn:std}
 \sigma = \sqrt{\frac{\sum_{i = 1}^{n} (\mu - L_i)^2}{n-1}}
\end{equation}


\subsection{The standard deviation test}
The mean and standard deviation can be used to detect outliers in a sample of data points drawn from an unknown population. The Gaussian distribution (also called the normal distribution) is used in association with the mean and the standard deviation. If the frequency which elements appear within a sample are more likely to be close to the mean than far away from it, we say that the elements within that sample are normally distributed. If an element differs from the mean by more than three standard deviations, the possibility of that element not belonging to that distribution is extremely high. Such elements, which likely do not belong to the population, are called outliers. The method which detects outliers by measuring the distance between each element and the mean is referred to, in this thesis, as the standard deviation test (SDT). The test assumes that elements within the collection are normally distributed with some mean $\mu_c$ and some standard deviation $\sigma_c$ (where c is the collection from which they are measured). By computing the z-score of the elements, the data is transformed into a standardized form through standardization. Z-score standardization subtracts the mean from all elements in the collection and divides the difference by the standard deviation, as expressed in equation (\ref{eqn:zscore}) \cite{geary1935ratio}.

\begin{equation}
\label{eqn:zscore}
 \frac{L_i - \mu_c} {\sigma_c}
\end{equation}

Following standardization, the entire sample will have a mean of zero and a standard deviation of one. Each element in the sample can then be measured using the standard deviation (one) as unit; outliers can then be discarded from the sample by removing elements which have an absolute value of 3 or higher.


\subsection{The interquartile range method}
An alternative method suited for outlier detection is the interquartile range method (IRM). IRM is based on analyzing the sample by its median, which is the center element of the sorted sample. It is applied by sorting the sample elements in ascending order and organizing the sample into four percentiles, each percentile containing $25\%$ of the sample. These percentiles are called the $q_{25}$, $q_{50}$, $q_{75}$, $q_{100}$ percentiles. Two consecutive quartiles center around $50\%$ of the sample contents, the $50\%$ in the center is referred to as the interquartile range. IRM requires two values from the sample: the highest value of the $25$th percentile and the highest value of the $75$th percentile. The former value is gained by taking the biggest value of the $q_{25}$ first elements from the sorted collection, where $q_{25}$ is the first $25\%$ of the number of elements in $L$ (calculated by $0.25 \cdot n$). The latter value is gained in a similar fashion, with the exception that the highest value is taken from the first $75\%$ of the sorted data. Two \textit{cut-off} points are then defined, by multiplying each of the two values by a constant $k$ (which is called the \textit{cut-off} constant). Elements can then be labeled as outliers if those elements fall below the lower \textit{cut-off} point or above the higher \textit{cut-off} point. \cite{vinutha2018detection, walfish2006review, dovoedo2015boxplot}.

\subsection{Analysis of variance}

The analysis of variance (ANOVA) is a method for statistical analysis developed by Ronald Fisher to analyze the difference among sample means in a collection of samples. It is based on the null-hypothesis stating that the sample means of two or more samples are the same. The analysis then yields a F-value and a p-value which are used with the F-distribution to accept or reject the null-hypothesis. The analysis assumes the population samples drawn are normally distributed and that the samples are independent of each other. It also assumes the standard deviations and variances are roughly equal among the samples \cite{scheffe1999analysis}.

The analysis is performed by calculating the mean of each sample. The summed square of differences from each mean are then calculated for their respective sample, the result is then subtracted by the squared sum of the elements within the sample divided by the number of elements in said sample. This calculation for sample $S$ is formally expressed in equation (\ref{eqn:SS}), where $\mu_S$ is the mean of sample $S$.

\begin{equation}
\label{eqn:SS}
 \sum_{s \in S} (s - \mu_S)^2 - \frac{(\sum_{s}^{S} s)^2}{|S|} 
\end{equation}

This is performed once for all samples drawn from the population, the results are then summed together into a sum of sample sums (SS). The calculation is also performed once on the total collection of all elements from all samples, the result is stored in the total sum of sample sums (SST). The total sum of squares is the sum of SS and the sum of squared distances from each sample mean to the total mean (SSM). SSM is therefore calculated by subtracting SST and SS. The analysis also requires divisions by two values called the degrees of freedom. They are calculated as follows: $d_1$ is the number of samples subtracted by one and $d_2$ is the number of elements in the total collection subtracted by the number of samples. The F-value is calculated by a fraction of two fractions. Let  $K_1$ be the fraction $\frac{SSM}{d_1}$ and $K_2$ be the fraction $\frac{SS}{d_2}$, the \textit{F-value} is defined as expressed in formula (\ref{eqn:Fval}).

\begin{equation}
\label{eqn:Fval}
 F = \frac{\frac{SSM}{d_1}}{\frac{SS}{d_2}} = \frac{K_1}{K_2}
\end{equation}

The quotient yielded by the fraction in formula (\ref{eqn:Fval}) can then be inserted into a pre-calculated table of the F-distribution using the degrees of freedom to yield the p-value. A small p-value (i.e. lower than $0.05$) indicates that the null-hypothesis may be rejected with relative certainty whereas a high p-value indicates the null-hypothesis holds and should be accepted with relative certainty \cite{lowry2014concepts}.


\section{Machine Learning}

Machine learning is the practice of computing models for relationships between sets of data. The field has garnered significant interest within academia and industry alike due to the promising results in applications for which deterministic algorithms have proven difficult or impossible to make. Examples of such applications are computer vision, natural language processing and personalized advertising, to name a few \cite{sebe2005machine, allen2003natural, malheiros2012too}. There are two main paradigms for learning: Supervised learning (using labeled data to approximate models) and unsupervised learning (finding patterns within the data itself). 

Models are used to great length within many scientific domains. In the context of machine learning, a model can be seen as a data structure made out of constant parameters combined with an algorithm which utilizes the data structure to produce predictions given an input vector (the input can also be in the form of a multi-dimensional matrix). 

The model can be represented mathematically as a collection of structures in the form of vectors or matrices, the elements of which are referred to as parameters. A model can consist of learnable parameters $\theta$ and non-learnable parameters (often generated by stochastic initialization if used). The model computes a function $f$ to yield a prediction $y$ by applying the algorithm to the parameters given an input example $x$ (which can be a vector or a matrix) drawn from the data set $X$. Let the dimensionality of the input $x$ be equal to the dimensionality of $\theta$. An example of a model prediction, where the algorithm produces predictions through addition, is given by formula (\ref{eqn:simplemodel}).

\begin{equation}
\label{eqn:simplemodel}
 y = x_0\theta_0 + x_1\theta_1 + ... + x_n\theta_n
\end{equation}

Machine learning then, is the practice of changing (also known as tuning) $\theta$ by introducing small changes to the elements within $\theta$. This is done to minimize a loss function which computes the error (or loss) given $y$. The process of changing $\theta$ is known as the training process and is central to machine learning. 

In the training process for supervised learning, the data gathered for the model is separated into three sets. These sets are referred to as the training set, validation set and test set. They are randomly collected samples from the common data set such that the intersection between any two sets is empty. The purpose of the training process is to train the model on the training data and use the validation and test sets as a means to validate the model performance on data not encountered during the training process. Supervised learning requires that the examples used have a label which the model tries to predict (data which possess labels are called labeled data). We say that a model generalizes well to the data if the training process allows the model to perform well on unseen data. If the model manages to perform well on the training set but fails to generalize, the model is said to overfit to the data. 

Unsupervised learning is a learning paradigm which does not rely on the use of labeled data. Instead, the paradigm focuses on organizing the data in a way that minimizes the loss function. Predictions can then be performed by evaluating the way the data has been organized by some method related to the problem context. The problem context is usually framed by two problem categories. These categories are regression and classification. Regression is the task of predicting continuous values given either continuous or discrete data i.e. predicting stock prices given information about the current economical situation or predicting the number of patients in a hospital during a pandemic given the density of the population. Classification aims to group different examples into categories (or classes) i.e. predicting whether an image contains a dog or a cat or which category a given tumor belongs to. Both supervised and unsupervised learning are used in this project.


\subsection{K-means Clustering}

Clustering is an unsupervised learning method whose primary use is in grouping data into sets. In this thesis we consider the \textit{K-means} clustering algorithm. The following is a formal definition of \textit{K-means} clustering as defined by MacQueen \cite{macqueen}. Given a set $E_n$ of $n$-dimensional points (where $n \in \mathbb{N}$) and a desired amount of partitions $k$ of $E_n$, partition the elements of $E_n$ into $k$ sets. The partitions are stored in a superset $S$ such that $S = \{S_1, S_2, ... S_k\}$. The partitioning of $E_n$ is performed by randomly initializing $k$ $n$-dimensional points as randomly selected points within $E_n$, these are the initial clusters. We define the set of clusters $V$ with elements $v$, where $v_i$ is the i:th cluster center and $i \in [1, k] \cap \mathbb{N}$. The partitioning of the elements $x \in E_n$ into their respective partition $S_i$ is performed by computing the closest cluster center for all elements in $E_n$. Let $T_i$ where $i \in [1, k] \cap \mathbb{N}$ be the set of elements $x \in E_n$ such that the distance from the element to the relevant cluster center is minimal; $T_i$ is defined by formula (\ref{eqn:Ti}).

\begin{equation}
\label{eqn:Ti}
T_i = \{x : x \in E_n | (|x - v_i| \leq |x - v_j|)\} (j \in [1, k] \cap \mathbb{N})
\end{equation}

For centers that share equal distance to any given $x$, the cluster with the smallest index is chosen as the containing set. This is performed by iteratively defining $S_i$ as the intersection of $T_i$ and the points which are not in any prior partitioned sets i.e. for $S_j$ where $j < i$. This is denoted by the set complement $S_i^c$ for all elements not in $S_i$. Let $S_1$ be defined by $T_1$, then the partitions $S_i \in S$ for $i \in [2, k] \cap \mathbb{N}$ are defined by formula (\ref{eqn:Si}).

\begin{equation}
\label{eqn:Si}
S_i = T_i \cap \bigcap_{j=1}^{(i-1)} S_j^c
\end{equation}

A consequence to this definition is that outliers have a potential to drastically change the quality of the clustering outcomes \cite{chawla2013k}. To remedy this and the stochastic nature of the initialization process, the method is run several times on the same dataset, yielding the optimal solution from those runs. This does not guarantee the best solution for the problem, but the solution is approximated. The problem \textit{K-means} clustering attempts to solve is proven to be NP-hard \cite{chawla2013k, mahajan2009planar} but the algorithm itself has a time complexity of $O(n^2)$ \cite{pakhira2014linear}.


\subsection{Hierarchical clustering}

Hierarchical clustering is a deterministic clustering method. Each cluster formed is based on the entire dataset, in contrast to \textit{K-means} which approximates clusters by performing small changes to the cluster centers. The method produces clusters by iteratively combining the closest clusters according to the given linkage criterion (defined in section 2.2.2.2). The two primary strategies for forming clusters are agglomerative and divisive. Agglomerative clustering initializes one cluster for each data point and combines them in a hierarchy according to the linkage criterion until all clusters are part of the hierarchy. Divisive strategies initializes one universal cluster for all data points and proceeds to separate the points into distinct clusters according to the linkage criterion. The method proceeds until all data points are separated to their own cluster within the unifying hierarchy. The project described in this thesis uses the agglomerative strategy. All strategies rely on specific distance metrics and linkage criteria \cite{murtagh1983survey}.

\subsubsection{Distance metrics}
Let $u$ and $v$ be vectors of the same dimension $n \in \mathbb{N}$. The \textit{Euclidean distance} (also called \textit{L2-distance}) metric can be used to measure distance between the vectors in Euclidean space. The \textit{Euclidean distance} between $u$ and $v$ is defined by formula (\ref{eqn:euclid}).

\begin{equation}
\label{eqn:euclid}
d(u, v) = \sqrt{\sum_{i=1}^n (u_i - v_i)^2} 
\end{equation}

The \textit{Manhattan distance} (also called \textit{L1-distance}) metric is also a viable alternative, if the distance is to be measured in blocks. The distance is akin to finding a shortest path among blocks and is therefore calculated as expressed in formula (\ref{eqn:manhattan}).

\begin{equation}
\label{eqn:manhattan}
d(u, v) = \sum_{i=1}^n |u_i - v_i|
\end{equation}

\textit{Cosine similarity} measures similarity between vector angles and suits situations where certain vectors are expected to be similar. Should the vectors be sizable in terms of dimensionality, this method will yield varying results, especially if the elements have vary significantly in each dimension. It is calculated as expressed in formula (\ref{eqn:cosine}).

\begin{equation}
\label{eqn:cosine}
d(u, v) = \frac{\sum_{i=1}^n u_iv_i}{\sqrt{\sum_{i=1}^n u_i^2}\sqrt{\sum_{i=1}^n v_i^2}}
\end{equation}

\subsubsection{Linkage Criteria}
In order to measure distance between clusters it is essential to know between which points the distance should be measured, since clusters often consist of several points. Linkage criteria describe the method for determining how the distance metric will be applied. In this project, we use the library SKlearn and the already defined methods within it to perform our analyses; the following criteria are therefore the only focus for this subsection. SKlearn defines four criteria in the documentation: Single linkage, complete linkage, average linkage and ward linkage \cite{scikit}. Depending on which criterion is applied, the results may differ considerably.

Single linkage goes through each pair of clusters measuring the distance among all points within one with respect to the other. The distance between these clusters is determined to be the distance between the two closest points. Let $U$ be the elements in the first cluster and $V$ be the elements of the second. The distance between the first and the second cluster is defined formally in formula (\ref{eqn:single}).

\begin{equation}
\label{eqn:single}
d(U, V) = \forall_{u, v \in U, V} min(d(u, v))
\end{equation}

Single linkage tends to produce trivial results, forging a hierarchy in a chain where individual elements slowly merge with the bigger cluster. In contrast, complete linkage considers the largest distance between two points for every pair of clusters. The distance between two clusters then becomes the distance between the points which are the furthest apart, formally expressed in formula (\ref{eqn:complete}).

\begin{equation}
\label{eqn:complete}
d(U, V) = \forall_{u, v \in U, V} max(d(u, v))
\end{equation}

By considering the largest possible distance between two clusters, this criterion bypasses the setback of single linkage, allowing more clusters to form before merging into one unifying cluster.

Average linkage calculates the average between all elements for every pair of clusters and merges the ones possessing minimal average distance. It is formally described by formula (\ref{eqn:average}).

\begin{equation}
\label{eqn:average}
d(U, V) = \frac{1}{|U||V|}\sum\limits_{u\in U} \sum\limits_{v\in V}  d(u, v)
\end{equation}

Ward linkage represents distance by how much the summed square would increase by merging them. The method aims to merge the clusters such that the within cluster variance is minimal. Let $c_a$ be the center of cluster \textit{a}, then ward linkage is expressed formally by formula (\ref{eqn:ward}) \cite{shalizi2009distances}.


\begin{equation}
\label{eqn:ward}
d(U, V) = \frac{|U||V|}{|U|+|V|}||c_U - c_V||^2
\end{equation}



\subsection{Feature Selection}

In many cases, the data available contains numerous features; e.g. different frequencies on a spectrum, which often helps to build sufficient classifiers, as the model may find non-trivial patterns among the features. To avoid expanding the dependence on large datasets and to minimize the computation time, it is often necessary to rid the data of certain features.  
Ideally, the features selected for removal are those which provide the least information or are completely uncorrelated with the subject under study. An example for such a feature would be the color of someones clothes correlated with the chances of said person seeing a squirrel on that day. In other words, features are removed if they possess minimal correlation to other features or lack correlation entirely \cite{dash1997feature}. Features that possess the necessary expressive information are not always trivial, and there are several ways in which they may be found. In this project we exclusively use one form of feature selection with the SKlearn library. The SelectKBest method is a method which ranks features by their significance according to some scoring function. In this project, we use the f-classif method to score the features in the data set. The method computes the F-value using ANOVA for each feature in the data provided, the features are then sorted according to the F-value after which, SelectKBest returns the $k$ features with the highest score.


\section{Deep Learning}

Deep Learning (DL) is part of machine learning and concerns the use of massive models. DL is commonly used in association with Artificial Neural Networks (or simply Neural Networks) which have been used to great success in classification and regression tasks alike. In this section, we review the preliminary methods central to Neural Networks in the context of DL. The concepts of activation functions, layers and optimizers are covered in context of what the project requires. 

\subsection{Neural Networks}
Neural Networks are machine learning models which have been used to great success during the $21$st century; in no small part due to the increase in computational power over the past decade. With the use of Neural Networks, several fields including Natural Language Processing, Encoding and Image classification have undergone revolutionary leaps in performance regarding optimization due to the predictive power of these networks \cite{sharir2020cost, kukacka2012overview, zhang2015deep, lee2017deep}. At the same time they are heavily criticized for their complexity, yielding a structure much more akin to a so-called \textit{"black box"} than a reliable and deterministic method for prediction. This complexity is due to numerous different structural typologies available at present and an awesome number of learnable parameters \cite{qiu2004opening}. A consequence of this is hard skepticism regarding the correctness of their functionality within practical use. While these models have shown great promise when compared to their human counterparts, the question remains whether or not perfect performance can be yielded from the constructed models.

A Neural Network consists of a set of learnable parameters $W_n$, $n = 1,...,k$ for a model possessing $k$ layers. These parameters are commonly referred to as weights and are matrices with arbitrary dimensionality, with a set number of parameters for each element in the input $x$. The first set of weights have one dimension set to the shape of $x$ and the other shapes are chosen according to the size of the layers specified by the user. The last set of weights $W_k$ has the size of the expected output signal i.e. the number of different categories available in the context of classification. The layers denote the size of the different shapes the input is transformed into as the input propagates through the architecture. The input is propagated through the architecture via the dot product of the weights and the layer signals, yielding a new vector of shape $l_n$. Layers can also be convolutional, meaning they are multi-dimensional structures which can be used in the context of image classification and Natural Language Processing, where input can be read in sequences, rather than giant data structures. This is managed by initializing smaller kernels which are able to compute signals from the input by only observing the defined size, they move over the entire input by steps called strides after which a pooling layer is used to summarize the final layer signal. Between each transformation, an activation function is used to transform the signal further in a non-linear fashion. Each layer transformation $f_n$ is then the yielded signal from the activation function given the dot product between the current and preceding layer. The signal of layer $l_i$ where $ 1 < i \leq k $ is then the result of the activation function $\sigma$ of the dot product of the preceding layer signal $l_{i-1}$ and the weights $W_i$. This layer function is denoted by the output of the nested function call of all functions $f_0, f_1, ... f_i$ of input $x$ as expressed in formula (\ref{eqn:prop}) \cite{wang2003artificial}.

\begin{equation}
\label{eqn:prop}
f_i(f_{i-1}(...f_1(x))) = l_i = \sigma(W_i \cdot l_{i-1} + b_i)
\end{equation}

The variable $b_i$ is the bias term for the activation. Its inclusion allows the activation curve to be moved along the x-axis. This shift in position of the activation allows the model to further change its own behavior through the learning process. It avoids bias towards the y-intercept of the activation function.  

\subsection{Activation functions}

Activation functions are used in neural networks to transform the input in a non-linear fashion. The function can be any function on numerical elements, the only requirement is that it must be derivable for all possible inputs. The functions usually transform the signal to be in a certain interval such as the hyperbolic tangent function (tanh) or the sigmoid function $\sigma$. The sigmoid function was originally used due to the similarities with the activation of biological neurons. The "s"-shaped curve of the function transforms any signal to the interval $[0, 1]$. It is comparable to tanh, which transforms signals into the interval $[-1, 1]$. The functions are formally expressed in formula (\ref{eqn:sig}) and (\ref{eqn:tanh}), respectively.

\begin{equation}
\label{eqn:sig}
\sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}

\begin{equation}
\label{eqn:tanh}
tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{equation}

Choosing an activation function depends on what range the user wants the signals to fall into. Sigmoid and tanh work sufficiently well for many models, giving promising results for many different tasks within DL. One flaw is that they are computationally expensive to calculate. The rectified linear unit (ReLU) is an activation function which is easily computed for many elements without the necessity for significant amounts of computational resources. The ReLU function returns the input itself if the input is greater than zero, and zero otherwise. The derivative of ReLU is similarly efficient to compute, the derivative is one for input greater than zero, and zero otherwise. The function is shown to outperform sigmoid and tanh as activation function in many cases, which has promoted its use in several applications. The drawback of ReLU is the derivative of 0 for signals of zero and below. The derivative is used during the training process to introduce changes to the model. With ReLU, the activation signal will become zero if the signal is smaller than zero. The neurons which suffer from this problem are referred to as "dead", since the weights used for their activation always bring the signal to or below zero. A possible fix for this is the leakyReLU function, which introduces a slight slope to the ReLU curve for values below zero. The ReLU function may then be defined through the leakyReLU function with a slope of zero for signals below zero. The derivative of leakyReLU then becomes one for values greater than zero, and a small real number for values below zero \cite{agarap2018deep, lu2019dying}.

The final activation function introduced in this section is the softmax function. Softmax is a method which transforms the signal in the output into a probability distribution i.e. scales the signals according to the maximum element and transforms the data to have a sum of one. This is done using Euler's constant $e$ as base, dividing $e$ raised to the power of each element in the input signal by the sum of all exponents. This is formally expressed in formula (\ref{eqn:softmax}) \cite{misra2019mish}.

\begin{equation}
\label{eqn:softmax}
\sigma(x)_i = \frac{e^{x_i}}{\sum_{j=1}^k e^{x_j}}
\end{equation}

\subsection{Regularization}
Regularization methods are used during the training process to aid in generalization for Neural Networks. One such method is dropout, which assigns a dropout rate to specifically selected layers. Dropout randomly reduces signals of individual neurons in the selected layers to zero which prevents the model from enforcing connections which become heavily affiliated with certain types of predictions. This is especially important in large architectures where layers can consist of hundreds of neurons where strong reinforcements are easily established \cite{srivastava2013improving}. Gaussian noise may also be added to all signals in any layer to shift the signals in them sporadically. However, this method requires some knowledge about the range of the signals within the layers, as large additive noise can remove any necessary information from the input which affects the gradient significantly. The model will learn to reduce dependency on noise during training, provided the noise is not "destructive". For example, adding Gaussian noise drawn from a normal distribution with a standard deviation of five to signals returned by the sigmoid activation function. This will shift the distribution of signals which removes valuable signal information. Batch normalization is a regularization technique which normalizes the change applied to $W_n$ during training over several batches of input and helps  regularizing the model \cite{santurkar2018does}.

\subsection{Optimization}

Neural Networks have many usable loss functions depending on context. In the context of classification with multiple categories, categorical cross-entropy is commonly used to measure error between the prediction of the network (usually produced with softmax or sigmoid activations) when the prediction is meant to categorize the input. The cross-entropy loss is calculated as the sum of negative elements of the true label $y^t$ multiplied by the logarithm of the predicted label $y$. Let $k$ be the number of elements in the output signal $y$, the cross-entropy loss is formally expressed in equation (\ref{eqn:crossent}).

\begin{equation}
\label{eqn:crossent}
-\sum_{i=1}^k y_{i}^t log(y_i)
\end{equation}

Equation (\ref{eqn:crossent}) measures the entropy between two distributions, entropy being the loss of information between two different distributions. The equation assumes the values within $y$ are elements of a probability distribution, i.e. the sum of the elements within $y$ equals one. The negation of the logarithm is used to bring the elements of the sum to a positive scope. This ensures the loss will be positive, since all elements in $y$ and $y^t$ are in the interval $[0, 1]$. The sigmoid function may also be used in association with cross-entropy, as the values will be transformed into the zero-to-one interval. Other activation functions such as tanh and ReLU run the risk of bringing elements in the sum to the negative scope or undefined (as the logarithm is undefined for values less than zero) \cite{krippendorff2009mathematical, shannon2001mathematical}.

The computed loss between the data labels and the predicted labels is then used by the optimizer to change the learnable parameters. The backpropagation algorithm calculates the partial derivatives of the computed loss with respect to each learnable parameter. The collection of the derived parameters are called the gradient. The gradient is scaled by the learning rate (usually a value less than $0.01$) defined in the optimizer, this allows for small changes to each parameter which allows the model to approach the minimum of the loss function at a speed proportional to the learning rate. Using the gradient in this way is called gradient descent and it is the common method of learning all optimizers use. Adam is an optimizer introduced by Kingma and Lei Ba \cite{kingma2014adam} which has proven to be efficient in contrast to other optimization methods such as Gradient Descent, AdaGrad and RMSprop. Adam uses an adaptive learning rate to approach the minimum of the loss function. Adam requires four different parameters for the algorithm to run. These are the learning rate $\alpha$, stochastic decay rates $\beta_1$ and $\beta_2$ and a small constant $\epsilon$ used to avoid division by zero for the update equation. The algorithm described by Kingma and Lei Ba also requires two vectors $m$ and $v$ used to describe the "moment" of the gradient and the squared gradient respectively (initialized as zero vectors). The gradient update at timestep $t$ is expressed by the following formulas:

\begin{center}


$t = t + 1$

$g_t = \nabla L_t(W_{t-1})$

$m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1) \cdot g_t $

$v_t = \beta_2 \cdot v_{t-1} + (1-\beta_2) \cdot g_t^2 $

$m_t' = \frac{m_t}{1-\beta_1^t}$

$v_t' = \frac{v_t}{1-\beta_2^t}$

$W_t = W_t - \alpha \cdot \frac{m_t'}{\sqrt{v_t'} + \epsilon}$

\end{center}

Each vector at timestep $t$ uses the values for the "moments" at the previous timestep $t-1$. Each "moment" is then updated using the previous timestep and the stochastic decay rates. Each "moment" vector is normalized by element-wise division of the "moment" vectors and their respective decay rates to the power of $t$. $\beta_1$ and $\beta_2$ are initialized to be $0.9$ and $0.999$ respectively, this ensures that subtraction by one will maintain the relative scope between the "moment" vectors and their corrected counterparts. The parameters themselves are then updated by subtracting the current parameters by alpha multiplied by the decay rate (calculated by the fraction of the "moment" vectors with the $\epsilon$ parameter added to the denominator). The "moment" vectors then give each feature a unique learning rate which accelerates training.