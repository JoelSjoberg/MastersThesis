In this chapter we present the necessary mathematical concepts on which this thesis is based. The concepts are considered fundamental for understanding the methods applied in the project. It begins by 
covering necessary mathematical theory required to understand data representations and handling within Machine Learning (ML). It then proceeds by defining common concepts in machine learning and the subject of supervised and unsupervised learning.


\section{Mathematical Foundations}

\section{Machine Learning}

Machine learning is the practice of computing models for relationships between sets of data. The field has garnered significant interest within academia and industry alike due to the promising result in applications for which deterministic algorithms have proven difficult or impossible to make. There are two paradigms for learning: Supervised learning (using labeled data to approximate models) and unsupervised learning (finding patterns within the data itself). 

Models are used to great length within many scientific domains. Though each domain has defined this term differently, the definitions in the context of machine learning shall be used. In this context, a model is a data structure made out of constant parameters which may be performed on any input vector $x$ to produce a prediction $y$

\vspace{5mm}

\textbf{Definition: } A model is an approximation of a desired function $f$ which produces relevant results based on human definitions.  

\vspace{5mm}

Mathematically a model may be represented as a collection of numbers $M$ which may in turn be used to compute $f$ for any given example.
In the context of machine learning a set of parameters may be tuned during a learning process (or training process). These parameters are combined with samples of data through some mathematical procedure to effectively model a distribution from which the data was extracted. The equation below is an example of a n-dimensional object.
$$x_0\theta_0 + x_1\theta_1 + ... + x_n\theta_n$$

\subsection{K-means Clustering}

Clustering is an unsupervised learning method whose primary use is in grouping sets of data. In this thesis we consider the \textit{K-means clustering} algorithm. The following is a formal definition of \textit{K - means clustering} as defined by MacQueen \cite{macqueen}. Given a set of $N$-dimensional points (where $N \in \mathbb{N}$) $E_N$ and a desired amount of partitions $k$ of $E_N$, partition the elements of $E_N$ into $k$ sets $S = \{S_1, S_2, ... S_k\}$. The partitioning of $E_N$ is performed by randomly initializing $k$  $N$-dimensional points as randomly selected points within $E_N$. We define the set $V$ with elements $v$ where $v_i$ is the i:th cluster center where $i \in [0, k]$. The partitioning of the elements $x \in E_N$ into their respective partition $S_i$ is performed by computing the closest cluster center $\forall_{x \in E_N}$. Let $T_i$ be the set of $x \in E_N$ such that the distance from the element to the relevant cluster is minimal, $T_i$ is defined by formula \ref{eqn:Ti}.

\begin{equation}
\label{eqn:Ti}
T_i = \{x : x \in E_N | (\forall_{j \in [0, k]/i } |x - v_i| \leq |x - v_j|)\} 
\end{equation}

For centers who share equal distance to any given $x$ the cluster with the smallest index is chosen as the containing set. The partitions $S_i \in S$ are defined by formula \ref{eqn:Si}

\begin{equation}
\label{eqn:Si}
S_i = T_i \cap \bigcap_{j=0}^{(i-1)} S_j^c
\end{equation}

A consequence to this definition is that outliers have a potential to drastically change the quality of the cluster outcomes\cite{chawla2013k}. To remedy this and the stochastic nature of the initialization process, the method is run several times on the same dataset, yielding the resulting clusters with minimal inertia. The problem \textit{K-means clustering} attempts to solve is proven to be NP-hard\cite{chawla2013k}\cite{mahajan2009planar} but the algorithm itself has a time complexity of $O(n^2)$\cite{pakhira2014linear}.


\subsection{Hierarchical clustering}

Hierarchical clustering is a clustering method which has a deterministic process. Each cluster formed is based on the entire dataset in contrast to \textit{K-means} which approximates clusters by performing small changes to the cluster centers. The method produces clusters by iteratively combining the closest clusters according to the given linkage criterion (defined in the sections below). The two primary strategies for forming clusters are \textit{agglomerative} and \textit{divisive}. Agglomerative clustering initializes one cluster for each data point and combines them in a hierarchy according to the linkage criterion until all clusters are part of the hierarchy. Divisive strategies initializes one universal cluster for all data points and proceeds to separate the points into distinct clusters according to the linkage criterion. The method proceeds until all data points are separated to their own cluster within the unifying hierarchy. The project described in this thesis uses the \textit{agglomerative} strategy. All strategies depend on the distance measure and linkage criterion \cite{murtagh1983survey}.

\subsubsection{Distance metrics}
Let $u$ and $v$ be vectors of the same dimension $n$. The \textit{Euclidean distance} (also called \textit{L2-distance}) measure can be used to measure distance between the vectors in euclidean space. The \textit{Euclidean distance} is calculated by equation (\ref{eqn:euclid}).

\begin{equation}
\label{eqn:euclid}
d(u, v) = \sqrt{\Sigma_i (u_i - v_i)^2} 
\end{equation}

The \textit{Manhattan distance} (also called \textit{L1-distance}) metric is also a viable alternative if the distance is to be measured in blocks. The distance is akin to finding a shortest path among blocks and is therefore calculated as expressed in equation (\ref{eqn:manhattan}).

\begin{equation}
\label{eqn:manhattan}
d(u, v) = \Sigma_i |u_i - v_i|
\end{equation}

\textit{Cosine similarity} measures similarity between vector angles and suits situations where certain vectors are expected to be similar. Should the vectors be sizable in terms of dimensionality, this method will yield varying results, especially if the elements have significant variance in each dimension. It is calculated as expressed in equation (\ref{eqn:cosine}).

\begin{equation}
\label{eqn:cosine}
d(u, v) = \frac{\Sigma_i u_iv_i}{\sqrt{\Sigma_i u_i^2}\sqrt{\Sigma_i v_i^2}}
\end{equation}

\subsubsection{Linkage Criteria}
In order to measure distance between clusters it is essential to know between which points the distance should be measured, since clusters often consist of several points. Linkage criteria describes the method for determining how the distance metric will be applied. SKlearn define four criteria in the documentation \cite{scikit}: Single linkage, complete linkage, average linkage and ward linkage. Depending on which criterion is applied the results may differ considerably, it is therefore vital to have a formal understanding of their application and consequences.

Single linkage goes through each pair of clusters measuring the distance among all points within one with respect to the other. The distance between these clusters is determined to be the distance between the two closest points. Let $U$ be the elements in the first cluster and $V$ be the elements of the second. The distance between the first and the second cluster is defined formally in equation (\ref{eqn:single}).

\begin{equation}
\label{eqn:single}
d(U, V) = \forall_{u, v \in U, V} min(d(u, v))
\end{equation}

Single linkage tend to produce trivial results, forging a hierarchy in a chain where individual elements slowly merge with the bigger cluster. In contrast complete linkage considers the largest distance between two points for every pair of clusters. The distance between two clusters then become the distance between the points which are the furthest apart. Formally expressed in equation (\ref{eqn:complete}).

\begin{equation}
\label{eqn:complete}
d(U, V) = \forall_{u, v \in U, V} max(d(u, v))
\end{equation}

By considering the largest possible distance between two clusters it bypasses the setback by single linkage, allowing more clusters to form before merging into one unifying cluster. Average linkage calculates the average between all elements for every pair of clusters and merges the ones possessing to the minimal average distance. Formally described by equation \ref{eqn:average}.

\begin{equation}
\label{eqn:average}
d(U, V) = \frac{1}{|U||V|}\sum\limits_u^U \sum\limits_v^V  d(U_u, V_v)
\end{equation}

Ward linkage represents distance by how much the summed square would increase by merging them. The method aims to merge the clusters such that the within cluster variance is minimal. Let $c_a$ be the center of cluster a, then ward linkage is expressed formally by equation \ref{eqn:ward}\cite{shalizi2009distances}.


\begin{equation}
\label{eqn:ward}
d(U, V) = \frac{|U||V|}{|U|-|V|}||c_U - c_V||^2
\end{equation}



\subsection{Feature Selection}

In many cases the data available contains numerous features, which often helps to building sufficient classifiers as the model may find non-trivial patterns among the features. To avoid expanding the dependence on large datasets, it is often necessary to strip the data of certain features which possess minimal correlation to other features or which lack that correlation entirely \cite{dash1997feature}. Features that possess the necessary expressive information are not always trivial, there are several ways in which they may be found. \textbf{TODO: Expand this, it's been 3 weeks already!}

\section{Deep Learning}

The field of Artificial Intelligence is founded on the notion of designing algorithms for solving problems. The field encountered tremendous progress in \textbf{[FIND YEAR, AI FOUNDATIONS]} referred to by \textbf{[NAME]} as the "look ma, no hands" era of Artificial Intelligence. One such method which have proven useful for these tasks is the practice of approximating models through Artificial Neural Networks.

\subsection{Artificial Neural Networks}
Artificial Neural Networks ("ANNs") have been used to great success during the 20th century [\textbf{Source Here}]. With the use of ANNs
several fields including Natural Language Processing, Encoding and Image classification have undergone revolutionary leaps in performance regarding optimization due to the predictive power of these networks [\textbf{Source Here}]. At the same time they are heavily criticized for their complexity, yielding a structure much more akin to a so called \textit{"black box"} than a reliable and deterministic method for prediction[\textbf{Source Here}]. This complexity is due to numerous different structural typologies available at present and an awesome number of tuned parameters which are modified with the goal of minimizing the predictive error [\textbf{Source Here}].

A consequence of this is hard skepticism in regards to the correctness of their functionality within practical use. While these models have shown great promise when compared to their human counterparts, the question remain whether or not perfect performance can be yielded from the constructed models.\\

\textbf{Definition 1.} Training an ANN is allowing minuscule changes through the randomly initialized structure in order to approximate a collection of nested functions $$f_n(f_{n-1}(...f_1(X)))$$ 

\subsection{Computational Representations}
The initial purpose of ANNs was to create a computational model of the human cortex which took the form of the McCulloch, Pitts neuron. The multilayer perceptron (MLP) introduced in \textbf{[year here]} formed the basic structure which would become ANNs. 


