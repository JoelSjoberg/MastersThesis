\begin{titlepage}
\vspace*{144pt}
\begin{center}
\Huge\bf Preparation of Raman spectra for classification methods in machine learning
%\Huge\bf Analysis and preparation of raman spectra for use in machine learning %------ NAME OF THE THESIS HERE!!!


\end{center}
\enlargethispage{3cm}
\vfill

\hfill
\begin{tabular}[t]{l@{}}%{\raggedleft%
\textit{Author:} Joel Sjöberg 38686\\ % YOUR NAME AND STUDENT ID!
Masters thesis in Computer Science\\ % BAC/MASTER THESIS AND MAIN SUBJECT
\textit{Supervisor:} Luigia Petre\\ % NAME OF YOUR SUPERVISOR
The Faculty Of Science And Engineering\\ % FACULTY
Åbo Akademi University\\ 
2021\\ % YEAR
\\
\\
\\
\\
\end{tabular}
%}%
\end{titlepage}


Machine learning is founded on statistical and linear algebraic theory. Moreover, certain methods within ML is heavily dependent on calculus to compute the change required to reach optimal solutions. The relevant theory for these concepts, accompanied by necessary examples will be covered in this section.

\subsection{Set Theory}

Set theory is a useful schematic originally presented by Georg Cantor in \textbf{[YEAR] [SOURCE HERE]}. Set theory concerns the theory of sets within the universe and allows for formalization of collections of elements e.g. the set of all natural numbers $\mathbb{N}$ is the set containing all whole numbers greater than or equal to 0. The cardinality of the set is $\infty$ as there is an infinite number of natural numbers expressed formally as \ref{eqn:card}.

\begin{equation}
\label{eqn:card}
|\mathbb{N}| = \infty
\end{equation}
The elements bellonging to a certain set is denoted by $2 \in \mathbb{N}$. 
Collections of sets bound by the operators $\cup$ (Union) and $\cap$ (intersection) produce sets of their own. Thus the following statements are theorems of set theory:


\begin{equation}
\label{eqn:union}
\mathbb{N} \cup \mathbb{Z} \equiv \mathbb{Z}
\end{equation}

\begin{equation}
\label{eqn:intersection}
\mathbb{N} \cap \mathbb{Z} \equiv \mathbb{N}
\end{equation}

Formally, the sets are collections of elements not limited to numbers, sets are primarily collections whose elements are devoid of order. Therefore we may have sets of items, , datatypes, people etc. The set devoid of elements is the empty set $\emptyset$. This set is in contrast to the universal set $U$ containing all elements in the universe. The complement of a set is the set containing all elements in the universe excluding the complemented set. Form this reasoning follows the following theorems of set theory.

\begin{equation}
\label{eqn:emptyC}
\emptyset^c  \equiv U
\end{equation}

\begin{equation}
\label{eqnuniverseC}
U^c  \equiv \emptyset
\end{equation}



\subsection{Statistics}

With ML we have the capability to analyze and develop models for systems or phenomena within them without rigorous definition of said systems. This may be achieved by gathering data which in some way describes the system in question. A collection of data is called a dataset, datasets consists of examples which may range from single valued numbers to multi-dimensional tensors. Formally we say a dataset $X$ is a subset of an unknown Population $X'$ which includes every possible example $x$.


Creating a model for predicting the systems behavior for the entire population is the goal of ML. Empirically it is rare to access data from the entire population $E$. Instead a subset of examples $x' \sim E_{X}$ is drawn from the population distribution $E_{X}$ for use within the ML-model. 

\subsection{Linear Algebra}

Linear algebra is founded on the theory of tensors. ML uses this theory immensely as the models produced in it are collections of numbers stored within matrices or multidimensional tensors. It is therefore vital to understand the preliminary concepts and terms within linear algebra.

A vector...

\subsection{Calculus}



In this section the data is presented in greater detail and prepared for the deep learning model. Due to the low number of samples available, it is necessary to examine each sample in detail to determine an appropriate strategy fit for deep learning. Each tumor present in the dataset is represented by a collection of Raman spectrum


The data must be sufficiently diverse between the given classes and similar within those classes for the predictive model to work appropriately. Should this not be the case, the model will struggle to reach desired performance by either failing to capture basic features of the data or by overfitting to it. The first part of this analysis examines the diversity among the sample-classes i.e. Whether the sample-classes are heterogeneous or homogeneous. This is important, since there must be a sufficient difference between the different classes to separate them appropriately. This also requires that the within-class samples are sufficiently similar i.e. All tumors belonging to the same class are homogeneous. Should this criterion not be satisfied the model is expected to overfit to the training data and fail to generalize to the test data. It is also necessary to prepare for eventual worst case scenarios. A possible worst case scenario is that all tumors are heterogeneous, which would mean that any model would be unable to satisfyingly distinguish between the classes. This means that we must evaluate the model on a patient by patient basis as the model would learn to recognize patterns in individual patients. This becomes relevant when selecting which samples to use for training the model and testing it. Should the tumors indeed be heterogeneous, it is ca  



Furthermore the algorithm uses a linkage criterion to decide which clusters to merge in the agglomerative method(the divisive version uses it to select clusters which should be separated). While many linkage criteria exist, only those available in SKlearns' implementation are explained in this section\cite{scikit}. These criteria consider the positional values of the elements within each cluster and with the distance metric, measures a distance between the clusters i.e. The linkage criterion decides the points between which the distance is measured.

Single linkage searches for the element in each cluster that are closest to the elements in every other cluster. The two clusters including the elements that are closest to each other are then merged, this criterion is formally expressed by equation \ref{eqn:single}.

 
 17.02.2021
To further alleviate memory issues, feature extraction is performed to reduce the size of the spectra to $(1, 70)$ by extracting the 70 features most capable of characterizing the data. This process is explained in detail in a later section. The provider further states that the spectra may be separated to individual spectrum as their alignment do not add any predictive power. This separation increase the dataset from $45$ patients to \textbf{(NUMBERHERE!!)} spectra, which is a sufficient number of datapoints for a deep learning model. Displaying each sample is possible by visualizing each spectra as a line, each patient has $w * h$ spectra. Before the lines are drawn the maximum absolute value of each frequency within the entire dataset is found. Using these, the frequencies of each spectra may be normalized to have a maximum value of 1 and a minimum value of -1. By normalization the plots may be compared to identify outliers and determine necessary preprocessing measures. Appendix \ref{appendix:spectraplot} contains a comparison by the common pattern these samples portray compared to one of the samples which will be removed from the dataset. This comparison is only one method of detecting outliers. The reason for this samples removal, along with the removal of other problematic samples is further motivated in the section below.
